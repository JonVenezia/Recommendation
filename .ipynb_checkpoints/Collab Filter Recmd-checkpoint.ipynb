{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import KNNBaseline\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Reader\n",
    "from surprise import get_dataset_dir\n",
    "import os\n",
    "from surprise.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import io\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. We will be using an algorithm that won the Netflix prize years back and leverages matrix factorization to recommend movies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best parameters for our model, we will begin with performing grid search to iterate over the possible paramters and cross validate models to find the best combination. \n",
    "\n",
    "Note 1: Due to randomness of the algorithm initialization, predictions vary with each run. \n",
    "Note 2: This model architecture is highly biased towards popular and highly rated movies which may make intuitive sense, but isn't likely to show people movies they haven't already considered watching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_epochs\": [20, 30, 40],\n",
    "    \"reg_all\": [0.02, 0.04, 0.08],\n",
    "    \"lr_all\": [0.002, 0.005]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])\n",
    "\n",
    "#best performance but I like the results given by the defaults parameters\n",
    "#0.9249955773675692\n",
    "#{'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.08}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning is very important. An initial model I trained with poorly tuned parameters ended up recommending the Wallace and Gromit film \"A Close Shave\" to most users https://www.imdb.com/title/tt0112691/\n",
    "\n",
    "Be careful with grid search however; depending on the combination of training dataset, algorithm, and most importantly the parameters you give it, grid search may optimize the objective and still produce some very odd and unhelpful results. If you give it a set of bad parameters, it can only select the best combination of a bad set of options. Always review the output and ask yourself, do these answers make sense? \n",
    "\n",
    "For instance, with this dataset, you can train a model with a low rmse but it leads to most people being recommended a small set of movies and in extreme cases, the same movie is recommended for nearly everyone. This happened with a little known movie known as Pather Panchali (1955) which has a high average rating, indicating it may be a very niche film and probably not a great recommendation for most people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Now we will load our dataset of movie ratings by users, train a model, and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Then predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = model.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=15)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's convert movie IDs to actual titles so we know what we are recommending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_item_names():\n",
    "    \"\"\"Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's define a new function to print a UID mapped to movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid_to_name, name_to_rid = read_item_names()\n",
    "def get_top_n_titles(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((rid_to_name[iid], est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_titles = get_top_n_titles(predictions, n=15)\n",
    "for uid, user_ratings in top_titles.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = []\n",
    "filmnames = []\n",
    "for uid, user_ratings in top_titles.items():\n",
    "    user.append(uid)\n",
    "    filist = [iid for (iid, _) in user_ratings]\n",
    "    #fname = [rid_to_name[i] for i in filist]\n",
    "    filmnames.append(\", \".join(filist))\n",
    "d = {\n",
    "    'User':user,\n",
    "    'Recommended Films':filmnames\n",
    "}\n",
    "filmDF = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN approach allows for findings similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train the algortihm to compute the similarities between items\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "knn = KNNBaseline(sim_options=sim_options)\n",
    "knn.fit(trainset)\n",
    "\n",
    "# Read the mappings raw id <-> movie name\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# Retrieve inner id of the movie\n",
    "def name_to_inner_id(filmname):\n",
    "    raw_id = name_to_rid[filmname]\n",
    "    return knn.trainset.to_inner_iid(raw_id)\n",
    "\n",
    "# Retrieve inner id of the movie with raw id\n",
    "def raw_to_inner_id(raw_id):\n",
    "    return knn.trainset.to_inner_iid(raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors to film.\n",
    "def get_neighbors(inner_id, n):\n",
    "    return knn.get_neighbors(inner_id, k=n)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "def get_neighbor_names(neighbors):\n",
    "    neighbor_names = (knn.trainset.to_raw_iid(inner_id) for inner_id in neighbors)\n",
    "    return (rid_to_name[rid] for rid in neighbor_names)\n",
    "\n",
    "def similar_movies(filmID, n):\n",
    "    m = raw_to_inner_id(filmID)\n",
    "    f = get_neighbors(m, n)\n",
    "    return get_neighbor_names(f)\n",
    "\n",
    "def similar_movies_by_name(film, n):\n",
    "    filmID = name_to_rid[film]\n",
    "    m = raw_to_inner_id(filmID)\n",
    "    f = get_neighbors(m, n)\n",
    "    return get_neighbor_names(f)\n",
    "\n",
    "neighbor_names = similar_movies_by_name('Dead Poets Society (1989)', 10)\n",
    "#print()\n",
    "print('The 10 nearest neighbors of Dead Poets Society (1989) are:')\n",
    "for movie in neighbor_names:\n",
    "    print(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe of users to top films suggested by KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = []\n",
    "filmnames = []\n",
    "for uid, user_ratings in top_n.items():\n",
    "    user.append(uid)\n",
    "    filist = [iid for (iid, _) in user_ratings]\n",
    "    fname = [rid_to_name[i] for i in filist]\n",
    "    filmnames.append(\", \".join(fname))\n",
    "d = {\n",
    "    'User':user,\n",
    "    'Recommended Films':filmnames\n",
    "}\n",
    "knnDF = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities: starter code to work with other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the code below to load a custom dataset from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to dataset file\n",
    "file_path = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/u.data')\n",
    "\n",
    "# As we're loading a custom dataset, we need to define a reader. In the\n",
    "# movielens-100k dataset, each line has the following format:\n",
    "# 'user item rating timestamp', separated by '\\t' characters.\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a model on a massive dataset of 25 million ratings (requires this dataset: https://grouplens.org/datasets/movielens/25m/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ml-25m/ratings.csv')\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['user id', 'item id', 'ratings']], reader)\n",
    "#data.split(2)  # data can now be used normally\n",
    "trainset = data.build_full_trainset()\n",
    "model25m = SVD()\n",
    "model25m.fit(trainset)\n",
    "\n",
    "# Then predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = model25m.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can explore our data by plotting the movies by average rating and number of ratings to find outliers and discover why our model may be more likely to recommend these movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"movielens data.xlsx\", sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=data['avg rating'],\n",
    "                                y=data['n ratings'],\n",
    "                                mode='markers',\n",
    "                                text=data['title']))\n",
    "fig.update_layout(title='Movie Scatterplot')\n",
    "fig.update_layout(\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=0.5,\n",
    "            y=-0.15,\n",
    "            showarrow=False,\n",
    "            text=\"Average Rating\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\"\n",
    "        ),\n",
    "        dict(\n",
    "            x=-0.07,\n",
    "            y=0.5,\n",
    "            showarrow=False,\n",
    "            text=\"Count of Ratings\",\n",
    "            textangle=-90,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
